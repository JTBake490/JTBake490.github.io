---
layout: post
title: Face the Ace
---

I have always been a tennis fan. Luckily for me, I grew up and continue to live in the golden era of tennis. I mostly cheered for Andy Roddick while he was still playing. Even now I get to enjoy Rodger Federer, Rafael Nadal, Novak Djokovic, the Bryan Brothers, and the William sisters, dominate the competition in their respective events. On top of that, there are many more world class professional tennis players that are putting on spectacular perfomances tournament after tournament. 

Face the Ace, was the first solo project I performed and the second Data Science project I participated in overall. The objective was to construct a model that could accurately predict the average aces per match a professional tennis player would hit in a 52 week period. After making a model, I wanted to identify the attributes that were most important for the prediction made. I knew that I wanted to include age, weight, and height. However, I wasn't sure about other statistics and headed over to the ATP and WTA websites to get a sense of what I should include in my model.

Luckily, the ATP and WTA publish service statistics on their website and I decided to use those statistics as my other features. For the service statistics I copied the stats and posted them into a spreadsheet. I then manually added the age, weight, and height for the men. Unfortunately, the weights for the women are not given on the WTA website. I then saved the file as a CSV and then read the data into a Pandas dataframe. Since the women had null values for the entire series (and even had a few missing height values) I decided to construct three models. One model would be for both genders, one for men only, and one for women only. After creating the standard linear models with cross-validation, I moved on to try two regularization models. I created ridge models and lasso models, also validated with cross-validation. I then moved on to try some featuring engineering.
![One indication that feature engineering might help](/images/tennis_model_images/lowess_1st_serve.svg) 

One indication that feature engineering might help was the lowess chart shown above.

Before putting the project away, I attempted to make a polynomial regression model work by fitting several degrees.

The results were mixed. A standard OLS linear regression model provided a R_squared of 0.709, 0.678, and 0.485 for combined genders, men, and women respectively. Not bad, but the lasso models helped improve the combined to 0.808 with a mean absolute error of 1.11, and improved the women's score to 0.572 with a mean absolute error of 0.668. The ridge model helped improve the men's score to 0.713 with a mean absolute error of 1.082. Glacing at the coefficients of my features, it became aparent that age is the weakest metric (lasso models even zeroed out the feature all together).

Some prelimanary exploratory data analysis indicated that feature engineering might be helpful. Since height and weight were provided for the men, I decided to include BMI as a metirc. I also decided to take the log of the 1st Serve Percentage Points Won series, as the Lowess Chart indicated that this might help. Unfortunately, all metrics suffered with these engineering steps. Finally, I fitted the datasets to a polynomial regression. The polynomial regressions with a degree of two yielded decent results, but were still worse than the lasso and ridge models.

In the end, I contructed three different models to predict a professional tennis player's ability to hit aces. I learned that most stats related to the serve play a role in the prediction (no surprise there). Height plays a minor role. Age had almost zero effet on the models. The R_squard for women was low across the board with a low mean absolute error and low mean squared error. This indicats that accurately predicting the average aces per match for professional women tennis players might be harder across the board. The biggest takeaway was that in some cases, the data provided might lead to the best metrics without feature engineering. I can imagine some Data Scientist might get carried away with feature enginerring in order to squeeze out and extra one percent improvement. As this project displayed, somethimes the extra effort goes unrewarded.